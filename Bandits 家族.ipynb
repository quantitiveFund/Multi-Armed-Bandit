{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ac0acd",
   "metadata": {},
   "source": [
    "# 强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20664b9d",
   "metadata": {},
   "source": [
    "## 定义："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f367a627",
   "metadata": {},
   "source": [
    "强化学习过程主要包含五个元素：智能体、环境、状态、动作、奖励。某一时刻下，智能体处于某一状态，执行一个动作后，环境接收到动作，促使智能体进入下一个状态，同时反馈奖励，智能体的目的是为了最大化累积奖励，根据奖励的多少调整动作以获得更大的奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7998ede",
   "metadata": {},
   "source": [
    "## MAB问题："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcbb4be",
   "metadata": {},
   "source": [
    "* 老虎机有K个摇臂，每个摇臂以一定的概率吐出金币，且概率是未知的\n",
    "* 玩家每次只能从K个摇臂中选择其中一个，且相邻两次选择或奖励没有任何关系\n",
    "* 玩家的目的是通过一定的策略使自己的奖励最大，即得到更多的金币\n",
    "\n",
    "合理的想法：对每个摇臂进行一定次数的尝试，得到每个摇臂的奖励均值，然后选择均值较大的摇臂，直到游戏结束。\n",
    "* 选择目前均值最大的摇臂进行游行称为“利用”(exploitation)\n",
    "* 选择其他的摇臂进行游戏称为“探索”(explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62222956",
   "metadata": {},
   "source": [
    "## 探索-利用困境:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11de608",
   "metadata": {},
   "source": [
    "在有限的游戏次数内获得最多的奖励。\n",
    "* 仅探索：将游戏机会平均分配给每个臂\n",
    "* 仅利用：将剩下的游戏机会全部选择当前已知的奖励最多的摇臂\n",
    "\n",
    "合理的想法：将一少部分的游戏次数用于“探索”，剩下的大部分游戏次数用于“利用”。\n",
    "可采用如下三种策略:\n",
    "* $ \\epsilon-greedy$ ：在游戏中设置一个探索率ε，以ε为概率进行探索；以概率1−ε 进行利用 **(探索完全随机)**\n",
    "\n",
    "\n",
    "* softmax : **核心思想为摇臂有多大可能性是最佳摇臂** (当前时刻摇臂的收益概率。收益概率越高，选择的概率越高) **（探索依概率随机）**\n",
    "$$p(a) = \\frac{e^\\frac{Q(a)}{\\tau}}{\\sum_{i=1}^{K}e^\\frac{Q(i)}{\\tau}}$$\n",
    "\n",
    "\n",
    "* UCB : **不确定条件下的乐观主义原则**。Q(a)为当前摇臂的奖励均值,后一项衡量置信度,t代表当前游戏总次数，N(a)代表当前摇臂的操作次数 **（完全不使用随机）**\n",
    "\n",
    "\n",
    "$$UCB = Q(a) + \\sqrt\\frac{2log(t)}{N(a)}$$\n",
    "$$arm=argmax_a[UCB]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4691b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutil_Armed_Bandit:\n",
    "    def __init__(self, k, N):\n",
    "        # 初始化 k 个 摇臂\n",
    "        self.k = k\n",
    "        # 初始化游戏次数\n",
    "        self.N = N \n",
    "        # 初始化各摇臂期望奖励\n",
    "        self.expect_reward = np.zeros(k)\n",
    "        # 初始化各摇臂操作次数\n",
    "        self.action_counts = np.zeros(k)\n",
    "        # 初始化总奖励\n",
    "        self.total_reward = 0\n",
    "        # 各摇臂给出奖励的概率\n",
    "        self.real_reward = [0.1, 0.2, 0.3, 0.8, 0.4]\n",
    "        # 当前摇臂\n",
    "        self.current_arm = 1\n",
    "        \n",
    "    def choose_arm(self, strategy, **kwargs):\n",
    "        if strategy == \"epsilon_greedy\":\n",
    "            if np.random.uniform() > kwargs[\"epsilon\"]:\n",
    "                arm = np.argmax(self.expect_reward)\n",
    "            else:\n",
    "                arm = np.random.choice(self.k)\n",
    "        if strategy == \"softmax\":\n",
    "            arm_prob = np.exp(np.array(\n",
    "                self.expect_reward)/kwargs[\"tau\"])/np.exp(np.array(self.expect_reward)/kwargs[\"tau\"]).sum()\n",
    "            arm = np.random.choice(self.k, p = arm_prob)\n",
    "    \n",
    "        return arm\n",
    "    \n",
    "    def update_arm(self, strategy, **kwargs):\n",
    "        for i in range(self.N):\n",
    "            if strategy == \"epsilon_greedy\":\n",
    "                arm = self.choose_arm(\"epsilon_greedy\", epsilon=kwargs[\"epsilon\"])\n",
    "            if strategy == \"softmax\":\n",
    "                arm = self.choose_arm(\"softmax\", tau = kwargs[\"tau\"])\n",
    "       \n",
    "            \n",
    "            self.current_arm = arm\n",
    "            # 所选择的摇臂反馈的奖励\n",
    "            arm_reward = np.random.binomial(1, self.real_reward[self.current_arm],1)\n",
    "            # 更新总奖励\n",
    "            self.total_reward += arm_reward\n",
    "            # 更新每个摇臂的期望奖励\n",
    "            self.expect_reward[self.current_arm] = (self.expect_reward[\n",
    "                self.current_arm] * self.action_counts[self.current_arm] + arm_reward)/(\n",
    "                self.action_counts[self.current_arm] + 1)\n",
    "            # 更新每个摇臂的操作次数\n",
    "            self.action_counts[self.current_arm] += 1\n",
    "        return self.total_reward, self.expect_reward, self.action_counts\n",
    "    \n",
    "    def ucb(self):\n",
    "        for i in range(self.N):\n",
    "            max_upper_bound = 0\n",
    "            for j in range(self.k):\n",
    "                if (self.action_counts[j] > 0): \n",
    "                    # 计算探索程度的大小\n",
    "                    delta_i = np.sqrt(2 * np.log(i + 1) / self.action_counts[j])\n",
    "                    # 计算UCB值\n",
    "                    upper_bound = self.expect_reward[j] + delta_i\n",
    "                else:\n",
    "                    # 初始化UCB值\n",
    "                    upper_bound = 1e400\n",
    "                if upper_bound > max_upper_bound:\n",
    "                    # 更新UCB值\n",
    "                    max_upper_bound = upper_bound\n",
    "                    # 选择最佳摇臂\n",
    "                    arm = j   \n",
    "            self.current_arm = arm\n",
    "            # 所选择的摇臂反馈的奖励\n",
    "            arm_reward = np.random.binomial(1, self.real_reward[self.current_arm],1)\n",
    "            # 更新总奖励\n",
    "            self.total_reward += arm_reward\n",
    "            # 更新每个摇臂的期望奖励\n",
    "            self.expect_reward[self.current_arm] = (self.expect_reward[self.current_arm] * self.action_counts[\n",
    "                self.current_arm] + arm_reward)/(self.action_counts[self.current_arm] + 1)\n",
    "            # 更新每个摇臂的操作次数\n",
    "            self.action_counts[self.current_arm] += 1\n",
    "        return self.total_reward, self.expect_reward, self.action_counts\n",
    "    \n",
    "    def choose_ucb_arm(self, iters):\n",
    "        if iters < self.k:\n",
    "            return iters\n",
    "        else:\n",
    "            for k in range(self.k):\n",
    "                upper_bound = np.sqrt(2 * np.log(k+1) / self.action_counts[k])\n",
    "                self.expect_reward[k] += upper_bound\n",
    "                arm = np.argmax(self.expect_reward)\n",
    "                return arm\n",
    "    \n",
    "    def excecute_ucb(self):\n",
    "        for i in range(self.N):\n",
    "            arm = self.choose_ucb_arm(i)\n",
    "            self.current_arm = arm\n",
    "            arm_reward = np.random.binomial(1, self.real_reward[self.current_arm],1)\n",
    "            # 更新总奖励\n",
    "            self.total_reward += arm_reward\n",
    "            # 更新每个摇臂的期望奖励\n",
    "            self.expect_reward[self.current_arm] = (self.expect_reward[self.current_arm] * self.action_counts[\n",
    "                self.current_arm] + arm_reward)/(self.action_counts[self.current_arm] + 1)\n",
    "            # 更新每个摇臂的操作次数\n",
    "            self.action_counts[self.current_arm] += 1\n",
    "        return self.total_reward, self.expect_reward, self.action_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1556d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([275]),\n",
       " array([0.125     , 0.28125   , 0.20754717, 0.76094276, 0.32608696]),\n",
       " array([ 40.,  64.,  53., 297.,  46.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epsilon_greedy\n",
    "mab = Mutil_Armed_Bandit(5,500)\n",
    "total_reward,expect_reward,action_counts = mab.update_arm(strategy = \"epsilon_greedy\",epsilon = 0.5)\n",
    "total_reward,expect_reward,action_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51a6d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([301]),\n",
       " array([0.12      , 0.12903226, 0.28571429, 0.81355932, 0.4       ]),\n",
       " array([ 25.,  31.,  49., 295., 100.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax\n",
    "mab = Mutil_Armed_Bandit(5,500)\n",
    "total_reward,expect_reward,action_counts = mab.update_arm(strategy = \"softmax\",tau = 0.3)\n",
    "total_reward,expect_reward,action_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ccde3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([378]),\n",
       " array([0.07142857, 0.27272727, 0.21052632, 0.85714286, 0.40625   ]),\n",
       " array([ 14.,  22.,  19., 413.,  32.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ucb\n",
    "mab = Mutil_Armed_Bandit(5,500)\n",
    "mab.ucb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479ca42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
